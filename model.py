# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QPnK5YOh8kRYPOOue6txwrgUqwKOMS0I
"""

# # Use seaborn for pairplot
# !pip install -q seaborn
# !pip install tensorflow==2.0.0
# # Use some functions from tensorflow_docs
# !pip install -q git+https://github.com/tensorflow/docs
# !pip install h5py pyyaml

from __future__ import absolute_import, division, print_function, unicode_literals

import pathlib


import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf
from tensorflow import keras

import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

# Commented out IPython magic to ensure Python compatibility.
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import ConnectionPatch
from collections import OrderedDict
from matplotlib.gridspec import GridSpec
from sklearn import metrics, linear_model
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, normalize
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from scipy.optimize import curve_fit
import warnings
plt.rcParams["patch.force_edgecolor"] = True
plt.style.use('fivethirtyeight')
mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)
# from IPython.core.interactiveshell import InteractiveShell
# InteractiveShell.ast_node_interactivity = "last_expr"
pd.options.display.max_columns = 50
# %matplotlib inline 
warnings.filterwarnings("ignore")
# import pickle

# create and save all the models
airlines = pd.read_csv('airlines.csv')
carriers = list(airlines['IATA_CODE'])
# print(carriers)
global train_stats
def norm(x):
  global train_stats
  return (x - train_stats['mean']) / train_stats['std']

def ret_stats():
  return train_stats
def build_model(train_ds):
  model = keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=[len(train_ds.keys())]),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
  ])

  optimizer = tf.keras.optimizers.RMSprop(0.001)

  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model
def do_create_models():
  for carrier in carriers:
    # create a model and save it for each carrier
    global train_stats
    df = pd.read_csv('carriers/carrier' + str(carrier) + 'data.csv')
    df.drop(['Unnamed: 0'], axis=1, inplace=True)

    # encode the origin 
    encoder = LabelEncoder()
    encoder.fit(df['ORIGIN_AIRPORT'])
    encoded_data_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))
    df['ORIGIN_AIRPORT'] = encoder.fit_transform(df['ORIGIN_AIRPORT'])

    # create the train and test dataset
    train_dataset = df.sample(frac=0.8,random_state=0)
    test_dataset = df.drop(train_dataset.index)

    # getting the stats
    train_stats = train_dataset.describe()
    train_stats.pop("ARRIVAL_DELAY")
    train_stats = train_stats.transpose()
    train_stats.to_csv('stats/train_stats' + str(carrier) + '.csv')
    # defining the train and test labels
    train_labels = train_dataset.pop('ARRIVAL_DELAY')
    test_labels = test_dataset.pop('ARRIVAL_DELAY')

    # normalize the data
    normed_train_data = norm(train_dataset)
    normed_test_data = norm(test_dataset)

    # # define the model
    # model = build_model(train_dataset)


    # # train the model
    # EPOCHS = 100
    # # The patience parameter is the amount of epochs to check for improvement
    # early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

    # early_history = model.fit(normed_train_data, train_labels, 
    #                     epochs=EPOCHS, validation_split = 0.2, verbose=0, 
    #                     callbacks=[early_stop, tfdocs.modeling.EpochDots()])
    # # calculating the loss
    # loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)

    # # weights = model.get_weights()
    # # fpkl = open('drive/My Drive/pickle_models/model-' + str(carrier) + '-weights.pkl', 'wb')
    # # pickle.dump(weights, fpkl, protocol=pickle.HIGHEST_PROTOCOL)

    # print("Testing set Mean Abs Error: {:5.2f} minutes".format(mae))
    # model.save('models/model-' + str(carrier) + '.h5')
    print('OK ' + str(carrier))

# let's create the input pipeline
from datetime import datetime

def conv_to_datetime(str_):
    return datetime.strptime(str_, '%Y-%m-%d %H:%M:%S')

def conv_to_time(str_):
    return datetime.strptime(str_, '%H:%M:%S')

import datetime

def string_to_time(time_string):
    if pd.isnull(time_string):
        return np.nan
    else:
        if time_string == 2400:
            time_string  = 0
        time_string = "{0:04d}".format(int(time_string))
        time_ = datetime.time(int(time_string[0:2]), int(time_string[2:4]))
        return time_
def func(x):
    return x.hour * 3600 + x.minute * 60 + x.second

dayOfWeek = 6
airline = 'AA'
origin = 'LAX'
dest = 'SEA'
sd = 200
ddelay = -10
sa = 800
dist = 1200


do_create_models()
# global train_stats

# stats = ret_stats()
# print(stats)

def processInput(input_):
    global train_stats
    processed = []
    time_sd = string_to_time(np.int64(input_["sd"]))
    time_sa = string_to_time(np.int64(input_["sa"]))
    time_sd = func(time_sd)
    time_sa = func(time_sa)
    # encode airlines to their numbers
    df = pd.read_csv('carriers/carrier' + str(input_["carrier"]) + 'data.csv')
    df.drop(['Unnamed: 0'], axis=1, inplace=True)
    encoder = LabelEncoder()
    encoder.fit(df['ORIGIN_AIRPORT'])
    encoded_data_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))


    carrier = input_["carrier"]
    
    for carr_ in carriers:
      # create a model and save it for each carrier
      if carr_ == carrier:
        df = pd.read_csv('carriers/carrier' + str(carr_) + 'data.csv')
        df.drop(['Unnamed: 0'], axis=1, inplace=True)

        # encode the origin 
        encoder = LabelEncoder()
        encoder.fit(df['ORIGIN_AIRPORT'])
        encoded_data_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))
        # print(encoded_data_map)
        df['ORIGIN_AIRPORT'] = encoder.fit_transform(df['ORIGIN_AIRPORT'])

        # # create the train and test dataset
        # train_dataset = df.sample(frac=0.8,random_state=0)
        # test_dataset = df.drop(train_dataset.index)

        # # getting the stats
        # train_stats = train_dataset.describe()
        # train_stats.pop("ARRIVAL_DELAY")
        # train_stats = train_stats.transpose()

        # # defining the train and test labels
        # train_labels = train_dataset.pop('ARRIVAL_DELAY')
        # test_labels = test_dataset.pop('ARRIVAL_DELAY')

        # # normalize the data
        # normed_train_data = norm(train_dataset)
        # normed_test_data = norm(test_dataset)

        # # define the model
        # model = build_model(train_dataset)


        # # train the model
        # EPOCHS = 100
        # # The patience parameter is the amount of epochs to check for improvement
        # early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

        # early_history = model.fit(normed_train_data, train_labels, 
        #                     epochs=EPOCHS, validation_split = 0.2, verbose=0, 
        #                     callbacks=[early_stop, tfdocs.modeling.EpochDots()])
        # # calculating the loss
        # loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)

        # print("Testing set Mean Abs Error: {:5.2f} minutes".format(mae))
        # model.save('models/model-' + str(carrier) + '.h5')

        # weights = model.get_weights()
        # fpkl = open('model-' + str(carrier) + '-weights.pkl', 'wb')
        # pickle.dump(weights, fpkl, protocol=pickle.HIGHEST_PROTOCOL)
        # print('OK ' + str(carrier))

    origin = input_["origin"]
    ddelay = input_["ddelay"]
    origin_ = encoded_data_map[origin]
    dist = input_["dist"]
    weekday = input_["dayOfWeek"]
    input_ = {"time_insec_dep" : time_sd, "time_insec_arr": time_sa,
              "ORIGIN_AIRPORT": origin_, "DEPARTURE_DELAY": ddelay,
              "DISTANCE": dist, "weekday": weekday }
    
    df = pd.DataFrame([input_])
    df = norm(df)

    model = keras.models.load_model('models/model-' + str(carrier) +'.h5')
    print("OK")
    return df, model

# input_ = {
#           "dayOfWeek": dayOfWeek,
#           "carrier": airline, 
#           "origin": origin,
#           "sd": sd, 
#           "ddelay": ddelay,
#           "sa": sa,
#           "dist": dist
#          }
# test_input, model = processInput(input_)

# from google.colab import drive
# drive.mount('/content/drive')
# !ls

# test_predictions_input = model.predict(test_input).flatten()



# print("The delay is: ", test_predictions_input[0], " minutes")



