{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PmbsmV73sSF",
        "colab_type": "code",
        "outputId": "129c2336-dcb0-413a-b571-31a953a011ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install -q seaborn\n",
        "!pip install tensorflow==2.0.0\n",
        "# Use some functions from tensorflow_docs\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "!pip install h5py pyyaml "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 58kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (45.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9konIBl-JSs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYhSUKu331RG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import pathlib\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzF7SSmz4tjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from sklearn import metrics, linear_model\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, normalize\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from scipy.optimize import curve_fit\n",
        "import warnings\n",
        "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "plt.style.use('fivethirtyeight')\n",
        "mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
        "pd.options.display.max_columns = 50\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sshhDD3J4xX8",
        "colab_type": "code",
        "outputId": "f752457e-3f05-41f1-f63b-7e447f5ecd14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create and save all the models\n",
        "airlines = pd.read_csv('drive/My Drive/airlines.csv')\n",
        "carriers = list(airlines['IATA_CODE'])\n",
        "print(carriers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['UA', 'AA', 'US', 'F9', 'B6', 'OO', 'AS', 'NK', 'WN', 'DL', 'EV', 'HA', 'MQ', 'VX']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqtZOCL46yfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovUUq0oi7B7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(train_ds):\n",
        "  model = keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=[len(train_ds.keys())]),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae', 'mse'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uWdxFaW4rAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(train_ds):\n",
        "  model = keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=[len(train_ds.keys())]),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae', 'mse', 'accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8djGda8b5fr3",
        "colab_type": "code",
        "outputId": "c7021f10-dfb0-4f86-f8f5-c95ae2827790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for carrier in carriers:\n",
        "  # create a model and save it for each carrier\n",
        "  df = pd.read_csv('drive/My Drive/carriers/carrier' + str(carrier) + 'data.csv')\n",
        "  df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "  # encode the origin \n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(df['ORIGIN_AIRPORT'])\n",
        "  encoded_data_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "  df['ORIGIN_AIRPORT'] = encoder.fit_transform(df['ORIGIN_AIRPORT'])\n",
        "\n",
        "  # create the train and test dataset\n",
        "  train_dataset = df.sample(frac=0.8,random_state=0)\n",
        "  test_dataset = df.drop(train_dataset.index)\n",
        "\n",
        "  # getting the stats\n",
        "  train_stats = train_dataset.describe()\n",
        "  train_stats.pop(\"ARRIVAL_DELAY\")\n",
        "  train_stats = train_stats.transpose()\n",
        "\n",
        "  # defining the train and test labels\n",
        "  train_labels = train_dataset.pop('ARRIVAL_DELAY')\n",
        "  test_labels = test_dataset.pop('ARRIVAL_DELAY')\n",
        "\n",
        "  # normalize the data\n",
        "  normed_train_data = norm(train_dataset)\n",
        "  normed_test_data = norm(test_dataset)\n",
        "\n",
        "  # define the model\n",
        "  model = build_model(train_dataset)\n",
        "\n",
        "\n",
        "  # train the model\n",
        "  EPOCHS = 100\n",
        "  # The patience parameter is the amount of epochs to check for improvement\n",
        "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "  early_history = model.fit(normed_train_data, train_labels, \n",
        "                      epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
        "                      callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
        "  # calculating the loss\n",
        "  loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "  weights = model.get_weights()\n",
        "  fpkl = open('drive/My Drive/pickle_models/model-' + str(carrier) + '-weights.pkl', 'wb')\n",
        "  pickle.dump(weights, fpkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  print(\"Testing set Mean Abs Error: {:5.2f} minutes\".format(mae))\n",
        "  errors = open('drive/My Drive/errors/errors.txt', mode='a')\n",
        "  errors.write(str(carrier) + ':' + str(mae))\n",
        "  model.save('models/model-' + str(carrier) + '.h5')\n",
        "  print('OK ' + str(carrier))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:103.9297,  mae:7.4930,  mse:103.9296,  val_loss:98.3167,  val_mae:7.2643,  val_mse:98.3167,  \n",
            "..............................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "16245/16245 - 0s - loss: 100.4079 - mae: 7.2785 - mse: 100.4079\n",
            "Testing set Mean Abs Error:  7.28 minutes\n",
            "OK UA\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:70.1730,  mae:6.1054,  mse:70.1729,  val_loss:63.2326,  val_mae:5.6943,  val_mse:63.2326,  \n",
            "...............................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "5386/5386 - 0s - loss: 61.9396 - mae: 5.6330 - mse: 61.9396\n",
            "Testing set Mean Abs Error:  5.63 minutes\n",
            "OK AA\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:63.8610,  mae:5.5593,  mse:63.8609,  val_loss:50.9450,  val_mae:5.0280,  val_mse:50.9450,  \n",
            ".....................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "1440/1440 - 0s - loss: 65.8826 - mae: 5.3922 - mse: 65.8826\n",
            "Testing set Mean Abs Error:  5.39 minutes\n",
            "OK US\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:94.8740,  mae:7.1326,  mse:94.8740,  val_loss:70.2443,  val_mae:6.1412,  val_mse:70.2443,  \n",
            ".....................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "973/973 - 0s - loss: 67.7627 - mae: 6.0860 - mse: 67.7627\n",
            "Testing set Mean Abs Error:  6.09 minutes\n",
            "OK F9\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:75.7409,  mae:6.1963,  mse:75.7409,  val_loss:65.6162,  val_mae:5.7482,  val_mse:65.6162,  \n",
            "..............................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "2675/2675 - 0s - loss: 65.2930 - mae: 5.7576 - mse: 65.2930\n",
            "Testing set Mean Abs Error:  5.76 minutes\n",
            "OK B6\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:73.6164,  mae:6.1954,  mse:73.6164,  val_loss:70.2235,  val_mae:6.1229,  val_mse:70.2235,  \n",
            "..........................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "11142/11142 - 0s - loss: 70.4927 - mae: 6.0389 - mse: 70.4927\n",
            "Testing set Mean Abs Error:  6.04 minutes\n",
            "OK OO\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:72.9562,  mae:6.0440,  mse:72.9562,  val_loss:63.4989,  val_mae:5.5928,  val_mse:63.4989,  \n",
            "......................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "1326/1326 - 0s - loss: 53.2604 - mae: 5.2722 - mse: 53.2604\n",
            "Testing set Mean Abs Error:  5.27 minutes\n",
            "OK AS\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:84.6231,  mae:6.5626,  mse:84.6231,  val_loss:45.8492,  val_mae:4.8901,  val_mse:45.8492,  \n",
            "..............WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "814/814 - 0s - loss: 46.3541 - mae: 5.0219 - mse: 46.3541\n",
            "Testing set Mean Abs Error:  5.02 minutes\n",
            "OK NK\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:48.9338,  mae:4.9634,  mse:48.9338,  val_loss:44.5885,  val_mae:4.7769,  val_mse:44.5885,  \n",
            "...........................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "12742/12742 - 0s - loss: 44.9882 - mae: 4.7946 - mse: 44.9883\n",
            "Testing set Mean Abs Error:  4.79 minutes\n",
            "OK WN\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:75.3277,  mae:6.1980,  mse:75.3277,  val_loss:72.9659,  val_mae:6.2099,  val_mse:72.9659,  \n",
            ".....................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "15297/15297 - 0s - loss: 71.8483 - mae: 6.0610 - mse: 71.8483\n",
            "Testing set Mean Abs Error:  6.06 minutes\n",
            "OK DL\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:76.9076,  mae:6.3099,  mse:76.9075,  val_loss:71.1153,  val_mae:6.1495,  val_mse:71.1153,  \n",
            "...............................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "11663/11663 - 0s - loss: 73.5208 - mae: 6.1932 - mse: 73.5209\n",
            "Testing set Mean Abs Error:  6.19 minutes\n",
            "OK EV\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:36.0415,  mae:4.3176,  mse:36.0415,  val_loss:37.8727,  val_mae:4.0295,  val_mse:37.8727,  \n",
            "...............................................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "348/348 - 0s - loss: 27.9402 - mae: 3.1869 - mse: 27.9402\n",
            "Testing set Mean Abs Error:  3.19 minutes\n",
            "OK HA\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:81.9224,  mae:6.5803,  mse:81.9224,  val_loss:60.5424,  val_mae:5.7092,  val_mse:60.5424,  \n",
            ".........................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "2033/2033 - 0s - loss: 61.6490 - mae: 5.6318 - mse: 61.6490\n",
            "Testing set Mean Abs Error:  5.63 minutes\n",
            "OK MQ\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:105.3399,  mae:7.5806,  mse:105.3399,  val_loss:106.2189,  val_mae:6.9842,  val_mse:106.2189,  \n",
            ".....................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "495/495 - 0s - loss: 77.0328 - mae: 6.2374 - mse: 77.0328\n",
            "Testing set Mean Abs Error:  6.24 minutes\n",
            "OK VX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdehQLmn_U8J",
        "colab_type": "code",
        "outputId": "e770ddfc-f572-4978-daa0-782d1ae63634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "for carrier in carriers:\n",
        "  # create a model and save it for each carrier\n",
        "  df = pd.read_csv('drive/My Drive/carriers/carrier' + str(carrier) + 'data.csv')\n",
        "  df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "  # encode the origin \n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(df['ORIGIN_AIRPORT'])\n",
        "  encoded_data_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "  df['ORIGIN_AIRPORT'] = encoder.fit_transform(df['ORIGIN_AIRPORT'])\n",
        "\n",
        "  # create the train and test dataset\n",
        "  train_dataset = df.sample(frac=0.8,random_state=0)\n",
        "  test_dataset = df.drop(train_dataset.index)\n",
        "\n",
        "  # getting the stats\n",
        "  train_stats = train_dataset.describe()\n",
        "  train_stats.pop(\"DEPARTURE_DELAY\")\n",
        "  train_stats = train_stats.transpose()\n",
        "\n",
        "  # defining the train and test labels\n",
        "  train_labels = train_dataset.pop('DEPARTURE_DELAY')\n",
        "  test_labels = test_dataset.pop('DEPARTURE_DELAY')\n",
        "\n",
        "  # normalize the data\n",
        "  normed_train_data = norm(train_dataset)\n",
        "  normed_test_data = norm(test_dataset)\n",
        "\n",
        "  # define the model\n",
        "  model = build_model(train_dataset)\n",
        "\n",
        "\n",
        "  # train the model\n",
        "  EPOCHS = 100\n",
        "  # The patience parameter is the amount of epochs to check for improvement\n",
        "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "  early_history = model.fit(normed_train_data, train_labels, \n",
        "                      epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
        "                      callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
        "  # calculating the loss\n",
        "  loss, mae, mse, acc = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "  weights = model.get_weights()\n",
        "  fpkl = open('drive/My Drive/pickle_models/model-' + str(carrier) + '-weights.pkl', 'wb')\n",
        "  pickle.dump(weights, fpkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  print(\"Testing set Mean Abs Error: {:5.2f} minutes\".format(mae))\n",
        "  model.save('models/model-' + str(carrier) + '.h5')\n",
        "  print('OK ' + str(carrier))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, accuracy:0.0285,  loss:58.3596,  mae:5.3021,  mse:58.3596,  val_accuracy:0.0303,  val_loss:50.6723,  val_mae:4.9660,  val_mse:50.6723,  \n",
            ".......................................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "16245/16245 - 0s - loss: 49.4905 - mae: 4.9166 - mse: 49.4905 - accuracy: 0.0307\n",
            "Testing set Mean Abs Error:  4.92 minutes\n",
            "OK UA\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, accuracy:0.0106,  loss:34.4828,  mae:3.8468,  mse:34.4827,  val_accuracy:0.0097,  val_loss:26.7904,  val_mae:3.5466,  val_mse:26.7903,  \n",
            "......................."
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3fe41b529433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   early_history = model.fit(normed_train_data, train_labels, \n\u001b[1;32m     39\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                       callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0;31m# calculating the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;31m# Copy saveable status of function's graph to current FuncGraph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0mdefault_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdefault_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m       \u001b[0mdefault_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_unsaveable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mbuilding_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3299\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbuilding_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m     \u001b[0;34m\"\"\"Returns True iff this graph represents a function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_building_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3303\u001b[0m   \u001b[0;31m# Helper functions to create operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S-aqLY79Gk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's create the input pipeline\n",
        "from datetime import datetime\n",
        "\n",
        "def conv_to_datetime(str_):\n",
        "    return datetime.strptime(str_, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "def conv_to_time(str_):\n",
        "    return datetime.strptime(str_, '%H:%M:%S')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzQZ_z_tAwal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "def string_to_time(time_string):\n",
        "    if pd.isnull(time_string):\n",
        "        return np.nan\n",
        "    else:\n",
        "        if time_string == 2400:\n",
        "            time_string  = 0\n",
        "        time_string = \"{0:04d}\".format(int(time_string))\n",
        "        time_ = datetime.time(int(time_string[0:2]), int(time_string[2:4]))\n",
        "        return time_\n",
        "def func(x):\n",
        "    return x.hour * 3600 + x.minute * 60 + x.second"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRtdoEL5AyLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dayOfWeek = 4\n",
        "airline = 'AA'\n",
        "origin = 'LAX'\n",
        "dest = 'PBI'\n",
        "sd = 2\n",
        "ddelay = -8\n",
        "sa = 750\n",
        "dist = 2330"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QMx269dA3Jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNrH8KYfA4sD",
        "colab_type": "code",
        "outputId": "ac564bc4-e0cb-4c86-aa16-c9b48b18804a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "global train_stats\n",
        "\n",
        "def processInput(input_):\n",
        "    global train_stats\n",
        "    processed = []\n",
        "    time_sd = string_to_time(np.int64(input_[\"sd\"]))\n",
        "    time_sa = string_to_time(np.int64(input_[\"sa\"]))\n",
        "    time_sd = func(time_sd)\n",
        "    time_sa = func(time_sa)\n",
        "    # encode airlines to their numbers\n",
        "    df = pd.read_csv('drive/My Drive/carriers/carrier' + str(input_[\"carrier\"]) + 'data.csv')\n",
        "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(df['ORIGIN_AIRPORT'])\n",
        "    encoded_data_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "\n",
        "\n",
        "    carrier = input_[\"carrier\"]\n",
        "    \n",
        "    for carr_ in carriers:\n",
        "      # create a model and save it for each carrier\n",
        "      if carr_ == carrier:\n",
        "        df = pd.read_csv('drive/My Drive/carriers/carrier' + str(carr_) + 'data.csv')\n",
        "        df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "        # encode the origin \n",
        "        encoder = LabelEncoder()\n",
        "        encoder.fit(df['ORIGIN_AIRPORT'])\n",
        "        encoded_data_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "        print(encoded_data_map)\n",
        "        df['ORIGIN_AIRPORT'] = encoder.fit_transform(df['ORIGIN_AIRPORT'])\n",
        "\n",
        "        # create the train and test dataset\n",
        "        train_dataset = df.sample(frac=0.8,random_state=0)\n",
        "        test_dataset = df.drop(train_dataset.index)\n",
        "\n",
        "        # getting the stats\n",
        "        train_stats = train_dataset.describe()\n",
        "        train_stats.pop(\"ARRIVAL_DELAY\")\n",
        "        train_stats = train_stats.transpose()\n",
        "\n",
        "        # defining the train and test labels\n",
        "        train_labels = train_dataset.pop('ARRIVAL_DELAY')\n",
        "        test_labels = test_dataset.pop('ARRIVAL_DELAY')\n",
        "\n",
        "        # normalize the data\n",
        "        normed_train_data = norm(train_dataset)\n",
        "        normed_test_data = norm(test_dataset)\n",
        "\n",
        "        # define the model\n",
        "        model = build_model(train_dataset)\n",
        "\n",
        "\n",
        "        # train the model\n",
        "        EPOCHS = 100\n",
        "        # The patience parameter is the amount of epochs to check for improvement\n",
        "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "        early_history = model.fit(normed_train_data, train_labels, \n",
        "                            epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
        "                            callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
        "        # calculating the loss\n",
        "        loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "        print(\"Testing set Mean Abs Error: {:5.2f} minutes\".format(mae))\n",
        "        # model.save('models/model-' + str(carrier) + '.h5')\n",
        "\n",
        "        weights = model.get_weights()\n",
        "        fpkl = open('model-' + str(carrier) + '-weights.pkl', 'wb')\n",
        "        pickle.dump(weights, fpkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print('OK ' + str(carrier))\n",
        "\n",
        "    origin = input_[\"origin\"]\n",
        "    ddelay = input_[\"ddelay\"]\n",
        "    origin_ = encoded_data_map[origin]\n",
        "    dist = input_[\"dist\"]\n",
        "    weekday = input_[\"dayOfWeek\"]\n",
        "    input_ = {\"time_insec_dep\" : time_sd, \"time_insec_arr\": time_sa,\n",
        "              \"ORIGIN_AIRPORT\": origin_, \"DEPARTURE_DELAY\": ddelay,\n",
        "              \"DISTANCE\": dist, \"weekday\": weekday }\n",
        "    \n",
        "    df = pd.DataFrame([input_])\n",
        "    df = norm(df)\n",
        "\n",
        "    model = keras.models.load_model('drive/My Drive/models/model-' + str(carrier) +'.h5')\n",
        "    print(\"OK\")\n",
        "    return df, model\n",
        "\n",
        "input_ = {\n",
        "          \"dayOfWeek\": dayOfWeek,\n",
        "          \"carrier\": airline, \n",
        "          \"origin\": origin,\n",
        "          \"sd\": sd, \n",
        "          \"ddelay\": ddelay,\n",
        "          \"sa\": sa,\n",
        "          \"dist\": dist\n",
        "         }\n",
        "test_input, model = processInput(input_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'10140': 0, '10257': 1, '10299': 2, '10397': 3, '10423': 4, '10529': 5, '10693': 6, '10713': 7, '10721': 8, '10792': 9, '10821': 10, '10994': 11, '11042': 12, '11057': 13, '11066': 14, '11109': 15, '11267': 16, '11278': 17, '11292': 18, '11298': 19, '11423': 20, '11433': 21, '11503': 22, '11540': 23, '11618': 24, '11638': 25, '11697': 26, '11884': 27, '11995': 28, '12173': 29, '12264': 30, '12266': 31, '12323': 32, '12339': 33, '12441': 34, '12451': 35, '12478': 36, '12758': 37, '12889': 38, '12892': 39, '12896': 40, '12953': 41, '12982': 42, '13198': 43, '13204': 44, '13230': 45, '13244': 46, '13256': 47, '13303': 48, '13342': 49, '13487': 50, '13495': 51, '13796': 52, '13830': 53, '13851': 54, '13871': 55, '13891': 56, '13930': 57, '13931': 58, '14027': 59, '14057': 60, '14100': 61, '14107': 62, '14122': 63, '14262': 64, '14307': 65, '14321': 66, '14492': 67, '14524': 68, '14570': 69, '14576': 70, '14635': 71, '14679': 72, '14683': 73, '14730': 74, '14747': 75, '14771': 76, '14831': 77, '14843': 78, '14869': 79, '14893': 80, '14908': 81, '15016': 82, '15024': 83, '15027': 84, '15096': 85, '15304': 86, '15370': 87, '15376': 88, 'ABQ': 89, 'ALB': 90, 'ANC': 91, 'ATL': 92, 'AUS': 93, 'BDL': 94, 'BHM': 95, 'BNA': 96, 'BOI': 97, 'BOS': 98, 'BUF': 99, 'BWI': 100, 'CHS': 101, 'CLE': 102, 'CLT': 103, 'CMH': 104, 'COS': 105, 'DAY': 106, 'DCA': 107, 'DEN': 108, 'DFW': 109, 'DSM': 110, 'DTW': 111, 'EGE': 112, 'ELP': 113, 'EWR': 114, 'FAT': 115, 'FLL': 116, 'GEG': 117, 'GSO': 118, 'GUC': 119, 'HDN': 120, 'HNL': 121, 'HOU': 122, 'IAD': 123, 'IAH': 124, 'ICT': 125, 'ILM': 126, 'IND': 127, 'JAC': 128, 'JAX': 129, 'JFK': 130, 'KOA': 131, 'LAS': 132, 'LAX': 133, 'LBB': 134, 'LGA': 135, 'LIH': 136, 'LIT': 137, 'MCI': 138, 'MCO': 139, 'MDT': 140, 'MEM': 141, 'MFE': 142, 'MIA': 143, 'MKE': 144, 'MSP': 145, 'MSY': 146, 'MTJ': 147, 'OAK': 148, 'OGG': 149, 'OKC': 150, 'OMA': 151, 'ONT': 152, 'ORD': 153, 'ORF': 154, 'PBI': 155, 'PDX': 156, 'PHL': 157, 'PHX': 158, 'PIT': 159, 'PNS': 160, 'PSP': 161, 'PVD': 162, 'PWM': 163, 'RDU': 164, 'RIC': 165, 'RNO': 166, 'ROC': 167, 'RSW': 168, 'SAN': 169, 'SAT': 170, 'SDF': 171, 'SEA': 172, 'SFO': 173, 'SJC': 174, 'SJU': 175, 'SLC': 176, 'SMF': 177, 'SNA': 178, 'STL': 179, 'STT': 180, 'STX': 181, 'SYR': 182, 'TPA': 183, 'TUL': 184, 'TUS': 185, 'XNA': 186}\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "\n",
            "Epoch: 0, loss:71.3519,  mae:6.1361,  mse:71.3520,  val_loss:63.2268,  val_mae:5.7292,  val_mse:63.2268,  \n",
            "...................................................WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "5386/5386 - 0s - loss: 61.5541 - mae: 5.6265 - mse: 61.5541\n",
            "Testing set Mean Abs Error:  5.63 minutes\n",
            "OK AA\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4T08CO5H1Au",
        "colab_type": "code",
        "outputId": "90a4dc18-0ea5-4a14-944f-3c7b22b62bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_VDyJF6BCEs",
        "colab_type": "code",
        "outputId": "c09d54a4-33fe-440d-abbc-cc3eaf624fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test_predictions_input = model.predict(test_input).flatten()\n",
        "test_predictions_input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-10.717558], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PNGThClCsvC",
        "colab_type": "code",
        "outputId": "03573543-bbd2-46af-84d6-0cef3da7364e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(test_predictions_input[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-10.717558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmWySvVDGy0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg7_M3c2G1sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}